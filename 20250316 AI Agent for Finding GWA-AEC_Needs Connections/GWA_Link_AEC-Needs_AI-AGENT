# THIS IS THE TINKER CODE FILE
import warnings
warnings.filterwarnings("ignore", category=Warning)  # Add this to handle SSL warnings

# Configuration flags
USE_OPENAI = True        # Set to False to use dummy responses instead of OpenAI
SKIP_CATEGORIZATION = False  # Set to True to skip categorizing leads
SKIP_STAKEHOLDER_MAP = True  # Set to True to skip creating stakeholder map - THIS DOESN'T SHOW MUCH WHEN THERE ARE 10+ COMPANIES
SKIP_PROPOSITIONS = True  # Set to True to skip generating propositions
SKIP_RELATIONS_CSV = True  # Set to True to skip creating company relations CSV

# Define constants
NUM_COMPANIES = 100  # Number of companies to search for

import os
from typing import List, Dict, Any
from langgraph.graph import Graph
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.utilities.google_search import GoogleSearchAPIWrapper
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import sys
from dotenv import load_dotenv, set_key
from pathlib import Path
from tqdm import tqdm
import time  # Add this import at the top
from concurrent.futures import ThreadPoolExecutor, as_completed

def get_dummy_response(prompt: str) -> str:
    """Generate dummy responses when OpenAI is disabled"""
    if "categorize" in prompt.lower():
        return "ConTech Startup"
    elif "connection" in prompt.lower() or "synergy" in prompt.lower():
        return "Yes"
    else:
        return "Sample proposition: Let's collaborate on a pilot project to implement your technology."

def setup_api_keys():
    """Setup API keys with user input if not found in environment"""
    # Load existing .env file or create if doesn't exist
    env_path = Path(get_script_dir()) / '.env'
    env_path.touch(exist_ok=True)
    load_dotenv(env_path)
    
    # Check for OpenAI API key
    if USE_OPENAI and not os.getenv('OPENAI_API_KEY'):
        print("\nâš ï¸ OPENAI_API_KEY not found in environment variables")
        api_key = input("Please enter your OpenAI API key: ").strip()
        set_key(env_path, 'OPENAI_API_KEY', api_key)
        os.environ['OPENAI_API_KEY'] = api_key
        print("âœ… OpenAI API key saved")

# Initialize API clients after setting up keys
def initialize_clients():
    clients = {}
    
    if USE_OPENAI:
        clients["llm"] = ChatOpenAI(model="gpt-4o-mini")
    else:
        print("â„¹ï¸ Using dummy responses instead of OpenAI")
        clients["llm"] = type('DummyLLM', (), {
            'invoke': lambda self, messages: type('DummyResponse', (), {'content': get_dummy_response(messages[-1].content)})()
        })()
    
    return clients

def get_script_dir():
    """Get the directory where the script is located"""
    return os.path.dirname(os.path.abspath(__file__))

class AECNetworkAgent:
    def __init__(self, clients):
        self.categories = {
            "ConTech Startup": [],
            "ConTech Investors": [],
            "ConTech Adopter": [],
        }
        self.connections = []
        self.llm = clients["llm"]
        self.output_dir = get_script_dir()
        
    def search_and_collect_data(self) -> Dict[str, List[Dict]]:
        print("\nðŸ” OpenAI Search Results:")
        print("==================================================")
        
        try:
            openai_prompt = f"""Please provide a list of {NUM_COMPANIES} real companies in total. Find approximately 1/3 for each category:
            1) startups, innovators, creators, technology suppliers, early stage companies, 
            2) corporate venture capital, private equity, venture capitalists, investors, 
            3) adopters, and 
            in the AEC (Architecture, Engineering, Construction) technology space.

            Use the following urls and to complete this search task.
            - https://contechroundup.substack.com/
            - https://builtworlds.com/
            - https://builtworlds.com/insights/2024-builtworlds-top-50-venture-investors/
            - https://builtworlds.com/insights/2024-top-50-venture-investments-deals/
            - https://builtworlds.com/insights/2025-building-tech-top-50-list/
            - https://builtworlds.com/insights/2024-adoption-leaders-50-list/
            - https://builtworlds.com/insights/2024-global-innovators-list-new/
            - https://builtworlds.com/insights/2025-preconstruction-tech-top-50-list/
            - https://www.constructiondive.com/
            - https://www.constructiondive.com/news/contech-startups-to-watch-2024/632886/
            - https://brickmortar.vc/portfolio 
            
            For each, include:
            Company: [name]
            Description: [description]
            URL: [url]
            
            Focus on companies involved in robotics, AI, and construction technology.
            Format each entry exactly as shown above, with each field on a new line."""
            
            print(f"Generating list of {NUM_COMPANIES} companies, this will take OpenAI a minute to complete, be patient... ")
            
            # Make the API call
            response = self.llm.invoke([
                SystemMessage(content="You are a construction technology industry expert with extensive knowledge of the AEC sector."),
                HumanMessage(content=openai_prompt)
            ])
            
            # Parse the response into structured data
            results = {"openai": []}
            company_data = self._parse_openai_response(response.content)
            if company_data:
                results["openai"].extend(company_data)
        
        except Exception as e:
            print("\nâŒ Error generating companies:", str(e))
            return {"openai": []}
        
        # Display results summary
        print("\nFound companies:")
        print("-" * 30)
        for i, company in enumerate(results["openai"], 1):
            print(f"\n{i}. {company.get('name', 'Unknown')}")
            print(f"   Description: {company.get('description', 'N/A')}")
            print(f"   URL: {company.get('url', 'N/A')}")
        
        print("\n" + "=" * 50)
        print(f"ðŸ“Š Using {len(results['openai'])} OpenAI-generated companies...")
        return results

    def _parse_openai_response(self, content: str) -> List[Dict]:
        """Helper method to parse OpenAI response into structured data"""
        companies = []
        current_company = {}
        
        lines = content.strip().split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # If we find a new company entry, save the previous one
            if line.lower().startswith("company:") and current_company:
                companies.append(current_company)
                current_company = {}
            
            # Parse the line
            if ":" in line:
                parts = line.split(":", 1)
                key = parts[0].strip().lower()
                value = parts[1].strip()
                
                if "company" in key:
                    current_company["name"] = value
                elif "description" in key:
                    current_company["description"] = value
                elif "url" in key:
                    current_company["url"] = value
        
        # Add the last company if exists
        if current_company:
            companies.append(current_company)
        
        return companies

    def categorize_leads(self, data: Dict[str, List[Dict]]) -> pd.DataFrame:
        categorized_data = []
        BATCH_SIZE = 5  # Process 5 companies at a time
        
        # Flatten the data into a single list
        all_companies = []
        for source, results in data.items():
            for result in results:
                if isinstance(result, str):
                    result = {"name": result, "description": result, "url": "N/A"}
                result["source"] = source
                all_companies.append(result)
        
        total_batches = (len(all_companies) + BATCH_SIZE - 1) // BATCH_SIZE
        
        with tqdm(total=len(all_companies), desc="Categorizing companies") as pbar:
            for i in range(0, len(all_companies), BATCH_SIZE):
                batch = all_companies[i:i + BATCH_SIZE]
                
                # Create batch prompt
                batch_prompt = "Categorize each company into exactly one category: ConTech Startup, ConTech Investors, or ConTech Adopter.\n\n"
                for idx, company in enumerate(batch, 1):
                    clean_name = company.get('name', 'Unknown').replace('*', '').strip()
                    batch_prompt += f"Company {idx}:\nName: {clean_name}\nDescription: {company.get('description', 'N/A')}\n\n"
                
                batch_prompt += "Return ONLY a comma-separated list of categories in order, e.g.: 'ConTech Startup, ConTech Investors, ConTech Adopter'"
                
                response = self.llm.invoke([
                    SystemMessage(content="You are a construction technology expert. Respond only with comma-separated categories."),
                    HumanMessage(content=batch_prompt)
                ])
                
                # Parse categories
                categories = [cat.strip() for cat in response.content.split(',')]
                
                # Add categorized companies to results
                for company, category in zip(batch, categories):
                    clean_name = company.get('name', 'Unknown').replace('*', '').strip()
                    categorized_data.append({
                        "Name": clean_name,
                        "Category": category,
                        "Description": company.get("description", ""),
                        "URL": company.get("url", "N/A"),
                        "Source": company.get("source", "unknown")
                    })
                    pbar.update(1)
        
        # Create DataFrame
        df = pd.DataFrame(categorized_data)
        return df[["Name", "Category", "Description", "URL", "Source"]]

    def create_stakeholder_map(self, df: pd.DataFrame) -> nx.Graph:
        G = nx.Graph()
        
        # Add nodes
        for _, row in df.iterrows():
            G.add_node(row["Name"], category=row["Category"])
        
        # Calculate total number of connections to check
        total_connections = sum(range(len(df)))
        
        # Use LLM to identify connections with progress bar
        with tqdm(total=total_connections, desc="Analyzing connections") as pbar:
            for i, row1 in df.iterrows():
                for j, row2 in df.iterrows():
                    if i < j:
                        connection_prompt = f"""
                        Analyze if there's a potential connection between these entities:
                        Entity 1: {row1['Name']} ({row1['Category']})
                        {row1['Description']}
                        
                        Entity 2: {row2['Name']} ({row2['Category']})
                        {row2['Description']}
                        
                        Return only 'Yes' or 'No'.
                        """
                        
                        response = self.llm.invoke([
                            SystemMessage(content="Analyze potential business connections."),
                            HumanMessage(content=connection_prompt)
                        ])
                        
                        if response.content.strip().lower() == "yes":
                            G.add_edge(row1["Name"], row2["Name"])
                        
                        pbar.update(1)
        
        return G

    def generate_propositions(self, df: pd.DataFrame) -> Dict[str, str]:
        propositions = {}
        relations_data = [] if not SKIP_RELATIONS_CSV else None  # Only create list if we're saving relations
        MAX_WORKERS = 3  # Limit concurrent API calls
        
        def process_company(lead_data):
            lead_idx, lead = lead_data
            
            # Get all potential connections in one API call
            companies_prompt = f"""
            Analyze potential synergies between {lead['Name']} ({lead['Category']}) and the following companies.
            Return ONLY the numbers (comma-separated) of companies that have synergy potential.
            
            Company details:
            {lead['Name']} ({lead['Category']}): {lead['Description']}
            
            Potential connections:
            """
            
            # Add numbered list of other companies
            other_companies = []
            for j, other_lead in df.iterrows():
                if lead_idx != j:
                    other_companies.append(f"{j+1}. {other_lead['Name']} ({other_lead['Category']}): {other_lead['Description']}")
            
            companies_prompt += "\n".join(other_companies)
            companies_prompt += "\n\nReturn ONLY the numbers of companies with synergy potential, comma-separated (e.g., '1,3,5')"
            
            # Get synergies
            response = self.llm.invoke([
                SystemMessage(content="You are a business synergy analyzer. Respond only with comma-separated numbers."),
                HumanMessage(content=companies_prompt)
            ])
            
            # Process results
            company_results = {
                'propositions': None,
                'relations': [] if not SKIP_RELATIONS_CSV else None  # Only track relations if needed
            }
            
            try:
                related_indices = [int(idx.strip()) - 1 for idx in response.content.strip().split(',') if idx.strip().isdigit()]
                related_companies = [df.iloc[idx] for idx in related_indices if 0 <= idx < len(df)]
                
                # Only store relations if we're not skipping the CSV
                if not SKIP_RELATIONS_CSV:
                    for related in related_companies:
                        company_results['relations'].append({
                            'Company1': lead['Name'],
                            'Category1': lead['Category'],
                            'Company2': related['Name'],
                            'Category2': related['Category']
                        })
                
                if related_companies:
                    proposition_prompt = f"""
                    Create a personalized LinkedIn message (max 300 characters) for {lead['Name']} that:
                    1. Mentions collaboration potential with {', '.join([comp['Name'] for comp in related_companies[:2]])}
                    2. Focuses on concrete business opportunities and mutual benefits
                    3. Includes a clear call to action
                    """
                    
                    prop_response = self.llm.invoke([
                        SystemMessage(content="Create concise, business-focused LinkedIn messages."),
                        HumanMessage(content=proposition_prompt)
                    ])
                    
                    company_results['propositions'] = (lead["Name"], prop_response.content)
                else:
                    company_results['propositions'] = (lead["Name"], f"No strong synergies identified for {lead['Name']} at this time.")
            
            except Exception as e:
                print(f"\nError processing {lead['Name']}: {str(e)}")
                company_results['propositions'] = (lead["Name"], f"Error generating proposition for {lead['Name']}")
            
            return company_results
        
        # Process companies in parallel
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_company = {
                executor.submit(process_company, (i, row)): row['Name'] 
                for i, row in df.iterrows()
            }
            
            with tqdm(total=len(df), desc="Generating propositions") as pbar:
                for future in as_completed(future_to_company):
                    company_name = future_to_company[future]
                    try:
                        result = future.result()
                        if result['propositions']:
                            propositions[result['propositions'][0]] = result['propositions'][1]
                        if not SKIP_RELATIONS_CSV and result['relations']:
                            relations_data.extend(result['relations'])
                    except Exception as e:
                        print(f"\nError processing {company_name}: {str(e)}")
                    pbar.update(1)
        
        # Save relations to CSV only if not skipped
        if not SKIP_RELATIONS_CSV and relations_data:
            relations_df = pd.DataFrame(relations_data)
            output_path = os.path.join(self.output_dir, 'company_relations.csv')
            relations_df.to_csv(output_path, index=False)
        
        return propositions

    def visualize_stakeholder_map(self, G: nx.Graph):
        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(G)
        
        # Draw nodes for each category with different colors
        colors = {
            "ConTech Startup": "skyblue",
            "ConTech Investors": "lightgreen",
            "ConTech Adopter": "salmon",
        }
        
        for category, color in colors.items():
            nodes = [node for node, attr in G.nodes(data=True) 
                    if attr.get("category") == category]
            nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=color, 
                                 node_size=1000, alpha=0.7)
        
        # Draw edges and labels
        nx.draw_networkx_edges(G, pos, alpha=0.5)
        nx.draw_networkx_labels(G, pos)
        
        # Add legend
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                         markerfacecolor=color, label=cat, markersize=10)
                         for cat, color in colors.items()]
        plt.legend(handles=legend_elements)
        
        plt.title("AEC Technology Stakeholder Network")
        plt.axis('off')
        plt.tight_layout()
        
        # Save to script directory
        output_path = os.path.join(self.output_dir, 'stakeholder_map.png')
        plt.savefig(output_path)
        plt.close()

def main():
    # Print configuration status
    print("\nðŸ”§ Configuration:")
    print(f"{'âœ…' if USE_OPENAI else 'âŒ'} OpenAI API: {'Enabled' if USE_OPENAI else 'Disabled'}")
    print(f"{'âŒ' if SKIP_CATEGORIZATION else 'âœ…'} Categorization: {'Skipped' if SKIP_CATEGORIZATION else 'Enabled'}")
    print(f"{'âŒ' if SKIP_STAKEHOLDER_MAP else 'âœ…'} Stakeholder Map: {'Skipped' if SKIP_STAKEHOLDER_MAP else 'Enabled'}")
    print(f"{'âŒ' if SKIP_PROPOSITIONS else 'âœ…'} Propositions: {'Skipped' if SKIP_PROPOSITIONS else 'Enabled'}")
    print(f"{'âŒ' if SKIP_RELATIONS_CSV else 'âœ…'} Relations CSV: {'Skipped' if SKIP_RELATIONS_CSV else 'Enabled'}")
    print()
    
    # Setup API keys and initialize clients
    setup_api_keys()
    clients = initialize_clients()
    
    # Create agent
    agent = AECNetworkAgent(clients)
    
    # Execute the workflow
    print("ðŸ” Searching for leads...")
    data = agent.search_and_collect_data()
    
    if not SKIP_CATEGORIZATION:
        print("ðŸ“Š Categorizing leads...")
        categorized_df = agent.categorize_leads(data)
        # Save to script directory
        output_path = os.path.join(agent.output_dir, 'categorized_leads.csv')
        categorized_df.to_csv(output_path, index=False)
    else:
        # Create a basic DataFrame without categorization
        categorized_data = []
        for source, results in data.items():
            for result in results:
                categorized_data.append({
                    "Name": result.get("name", "Unknown"),
                    "Category": "Uncategorized",
                    "Description": result.get("description", ""),
                    "URL": result.get("url", "N/A"),
                    "Source": source
                })
        categorized_df = pd.DataFrame(categorized_data)
        output_path = os.path.join(agent.output_dir, 'leads.csv')
        categorized_df.to_csv(output_path, index=False)
    
    if not SKIP_STAKEHOLDER_MAP:
        print("ðŸ•¸ï¸ Creating stakeholder map...")
        G = agent.create_stakeholder_map(categorized_df)
        agent.visualize_stakeholder_map(G)
    
    if not SKIP_PROPOSITIONS:
        print("ðŸ“ Generating propositions...")
        propositions = agent.generate_propositions(categorized_df)
        
        # Save propositions to script directory
        output_path = os.path.join(agent.output_dir, 'propositions.txt')
        with open(output_path, 'w') as f:
            for name, proposition in propositions.items():
                f.write(f"\n{'='*50}\n")
                f.write(f"Proposition for: {name}\n")
                f.write(f"{'='*50}\n")
                f.write(f"{proposition}\n")
    
    print(f"\nâœ… Process complete! Check output files in: {agent.output_dir}")
    if not SKIP_CATEGORIZATION:
        print("- categorized_leads.csv")
    else:
        print("- leads.csv")
    if not SKIP_STAKEHOLDER_MAP:
        print("- stakeholder_map.png")
    if not SKIP_PROPOSITIONS:
        print("- propositions.txt")
        if not SKIP_RELATIONS_CSV:
            print("- company_relations.csv")

if __name__ == "__main__":
    main()
